---
version: "3"

tasks:
  deploy-infrastructure:
    desc: "Deploy monitoring infrastructure"
    cmds:
      - ctlptl apply -f Cluster.yaml
      - helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
      - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
      - helm repo add grafana https://grafana.github.io/helm-charts
      - echo "üöÄ Starting monitoring infrastructure deployment..."
      - |
        helm upgrade --install \
          --create-namespace \
          --namespace kube-system \
          --set controller.progressDeadlineSeconds=500 \
          --version 4.12.2 \
          --wait \
          ingress-nginx ingress-nginx/ingress-nginx
      - |
        helm upgrade --install \
          --create-namespace \
          --namespace monitoring \
          --version 72.6.2 \
          --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \
          --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \
          --set-string prometheus.prometheusSpec.serviceMonitorNamespaceSelector.matchLabels.monitoring=true \
          --set prometheus.enabled=false \
          --set alertmanager.enabled=false \
          --set kubeStateMetrics.enabled=false \
          --set nodeExporter.enabled=false \
          --set grafana.enabled=false \
          --wait \
          prometheus-operator prometheus-community/kube-prometheus-stack
      - |
        helm upgrade --install \
          --create-namespace \
          --namespace monitoring \
          --version 5.18.0 \
          --wait \
          grafana-operator grafana/grafana-operator
      - kubectl label namespace monitoring monitoring="true" --overwrite
      - kubectl apply -f prometheus/
      - kubectl apply -f grafana/
      - echo "üìä Monitoring infrastructure deployed successfully!"

  deploy-inference-gateway:
    desc: "Deploy inference-gateway with monitoring and autoscaling enabled"
    cmds:
      - |
        helm upgrade --install \
          --create-namespace \
          --namespace inference-gateway \
          --set config.TELEMETRY_ENABLE=true \
          --set config.OLLAMA_API_URL="http://ollama.ollama:8080/v1" \
          --set monitoring.enabled=true \
          --set ingress.enabled=true \
          --set autoscaling.enabled=true \
          --set autoscaling.minReplicas=2 \
          --set autoscaling.maxReplicas=10 \
          --set autoscaling.targetCPUUtilizationPercentage=80 \
          --set autoscaling.targetMemoryUtilizationPercentage=80 \
          --version 0.14.0 \
          --wait \
          inference-gateway oci://ghcr.io/inference-gateway/charts/inference-gateway
      - kubectl label namespace inference-gateway monitoring="true" --overwrite
      - echo "üìà Inference Gateway with monitoring and autoscaling deployed successfully!"

  deploy-ollama:
    desc: "Deploy ollama as a provider"
    cmds:
      - kubectl create namespace ollama --dry-run=client -o yaml | kubectl apply -f -
      - |
        kubectl apply -f ollama/
      - kubectl -n ollama rollout status deployment ollama
      - echo "ü§ñ Ollama provider deployed successfully!"

  pull-models:
    desc: "Pull required models for testing"
    cmds:
      - echo "üì• Pulling required models..."
      - |
        kubectl exec -n ollama deployment/ollama -- ollama pull deepseek-r1:1.5b
      - |
        kubectl exec -n ollama deployment/ollama -- ollama pull llama3.2:1b
      - echo "‚úÖ Models pulled successfully!"

  simulate-requests:
    desc: "Generate test requests to simulate monitoring"
    cmds:
      - |
        for i in $(seq 1 20); do
          curl -s -X POST http://api.inference-gateway.local/v1/chat/completions \
            -H "Content-Type: application/json" \
            -d '{"model":"ollama/deepseek-r1:1.5b","messages":[{"role":"user","content":"Say hello"}]}' \
            --max-time 10
          echo
          sleep 1
        done
      - echo "üß™ Test requests completed successfully!"

  simulate-tool-call-requests:
    desc: "Generate test requests with tool/function calls to simulate advanced monitoring"
    cmds:
      - |
        for i in $(seq 1 10); do
          curl -s -X POST http://api.inference-gateway.local/v1/chat/completions \
            -H "Content-Type: application/json" \
            -d '{
              "model": "ollama/llama3.2:1b",
              "messages": [
                {
                  "role": "user",
                  "content": "What is the weather like in New York? Use the weather tool to check."
                }
              ],
              "tools": [
                {
                  "type": "function",
                  "function": {
                    "name": "get_weather",
                    "description": "Get the current weather for a location",
                    "parameters": {
                      "type": "object",
                      "properties": {
                        "location": {
                          "type": "string",
                          "description": "The city and state, e.g. San Francisco, CA"
                        }
                      },
                      "required": ["location"]
                    }
                  }
                }
              ],
              "tool_choice": "auto"
            }' \
            --max-time 15
          echo
          sleep 2
        done
      - echo "üõ†Ô∏è Tool call test requests completed successfully!"

  clean:
    desc: "Clean up the cluster"
    cmds:
      - ctlptl delete -f Cluster.yaml
      - echo "üßπ Cluster cleaned up successfully!"
