# Default values for inference-gateway.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: ghcr.io/inference-gateway/inference-gateway
  pullPolicy: IfNotPresent
  tag: ''

monitoring:
  enabled: false
  metricsPort: 9464
  namespaceSelector:
    matchNames:
      - monitoring

# Secrets and configurations references
envFrom:
  configMapRef:
  secretRef:

# This is for the secrets for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# This is to override the chart name.
nameOverride: ''
fullnameOverride: ''

# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ''

# This is for setting Kubernetes Annotations to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
podAnnotations: {}
# This is for setting Kubernetes Labels to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
podLabels: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
service:
  # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
  type: ClusterIP
  # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
  port: 8080

# This block is for setting up the ingress for more information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/
ingress:
  enabled: false
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: 'selfsigned-issuer'
  hosts:
    - host: api.inference-gateway.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls:
    enabled: false
    # Hosts can be specified as either:
    # hosts: api.inference-gateway.local  # Single host
    # or
    hosts:
      - api.inference-gateway.local
    secretName: api-inference-gateway-tls

resources:
  limits:
    cpu: 200m
    memory: 256Mi
  requests:
    cpu: 100m
    memory: 128Mi

# This is to setup the liveness and readiness probes more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
livenessProbe:
  httpGet:
    path: /health
    port: http
readinessProbe:
  httpGet:
    path: /health
    port: http

# This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}

# Extra environment variables to add to the deployment
extraEnv: []
# - name: EXAMPLE_VAR
#   value: "example-value"

config:
  # General settings
  ENVIRONMENT: 'production'
  ALLOWED_MODELS: ''
  DEBUG_CONTENT_TRUNCATE_WORDS: '10'
  DEBUG_MAX_MESSAGES: '100'
  # Telemetry
  TELEMETRY_ENABLE: 'false'
  TELEMETRY_METRICS_PORT: '9464'
  # Model Context Protocol (MCP)
  MCP_ENABLE: 'false'
  MCP_EXPOSE: 'false'
  MCP_SERVERS: ''
  MCP_CLIENT_TIMEOUT: '5s'
  MCP_DIAL_TIMEOUT: '3s'
  MCP_TLS_HANDSHAKE_TIMEOUT: '3s'
  MCP_RESPONSE_HEADER_TIMEOUT: '3s'
  MCP_EXPECT_CONTINUE_TIMEOUT: '1s'
  MCP_REQUEST_TIMEOUT: '5s'
  MCP_MAX_RETRIES: '3'
  MCP_RETRY_INTERVAL: '5s'
  MCP_INITIAL_BACKOFF: '1s'
  MCP_ENABLE_RECONNECT: 'true'
  MCP_RECONNECT_INTERVAL: '30s'
  MCP_POLLING_ENABLE: 'true'
  MCP_POLLING_INTERVAL: '30s'
  MCP_POLLING_TIMEOUT: '5s'
  MCP_DISABLE_HEALTHCHECK_LOGS: 'true'
  # Authentication
  AUTH_ENABLE: 'false'
  AUTH_OIDC_ISSUER: 'http://keycloak:8080/realms/inference-gateway-realm'
  # Server settings
  SERVER_HOST: '0.0.0.0'
  SERVER_PORT: '8080'
  SERVER_READ_TIMEOUT: '30s'
  SERVER_WRITE_TIMEOUT: '30s'
  SERVER_IDLE_TIMEOUT: '120s'
  SERVER_TLS_CERT_PATH: ''
  SERVER_TLS_KEY_PATH: ''
  # Client settings
  CLIENT_TIMEOUT: '30s'
  CLIENT_MAX_IDLE_CONNS: '20'
  CLIENT_MAX_IDLE_CONNS_PER_HOST: '20'
  CLIENT_IDLE_CONN_TIMEOUT: '30s'
  CLIENT_TLS_MIN_VERSION: 'TLS12'
  CLIENT_DISABLE_COMPRESSION: 'true'
  CLIENT_RESPONSE_HEADER_TIMEOUT: '10s'
  CLIENT_EXPECT_CONTINUE_TIMEOUT: '1s'
  # Providers
  ANTHROPIC_API_URL: 'https://api.anthropic.com/v1'
  CLOUDFLARE_API_URL: 'https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai'
  COHERE_API_URL: 'https://api.cohere.ai'
  GROQ_API_URL: 'https://api.groq.com/openai/v1'
  OLLAMA_API_URL: 'http://ollama:8080/v1'
  OPENAI_API_URL: 'https://api.openai.com/v1'
  DEEPSEEK_API_URL: 'https://api.deepseek.com'
  GOOGLE_API_URL: 'https://generativelanguage.googleapis.com/v1beta/openai'
  MISTRAL_API_URL: 'https://api.mistral.ai/v1'
